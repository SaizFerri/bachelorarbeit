\chapter{Implementierung}
In diesem Kapitel wird die Implementierung der Methode näher erläutert. Bei der Implementierung werden alle Konzepte aus dem
Kapitel Konzeption angewendet. Die Implementierung der Methode kann im Anhang oder auf dem folgenden GitHub Repository\footnote{\url{https://github.com/SaizFerri/colorization-pytorch-unet}} gefunden werden.

\section{Binning}
Das Binning wurde mit Hilfe der ``numpy'' Bibliothek für normalisierte Lab-Bilder implementiert.
Als erstes wird mit Hilfe der Wurzel
anhand der Anzahl von Bins ($n\_bins$) die Breite ($W$) und Höhe ($H$) des \gls{grid}s, das auf der Abbildung \ref{image:bins} zu sehen ist,
berechnet. Es wird angenommen dass $W = H$ ist.
Nachdem die Breite des \gls{grid}s berechnet wurde, wird der Intervall von $[0, 1]$ in $W$ gleich große Intervalle aufgeteilt. Als nächstes
wird mit Hilfe der \textit{digitize} Funktion der Intervall Index der jeweiligen $a, b$ Farbkanal Wert von jedem Pixel kalkuliert.
Abschließend werden beide Indices zu einem Bin Index auf dem \gls{grid} umgewandelt. Der Output ist ein kodiertes Bild mit einem Bin Index per Pixel.
\\

\begin{listing}[H]
  \begin{minted}{python}
    # a, b sind die Koordinaten der Farbkanäle auf dem Grid
    def calculate_bin(a, b, width):
      return (width * b) + a
  
    def encode_bins(ab_image, n_bins):
      W = np.sqrt(n_bins).astype(int)
  
      # Intervall in gleich große Intervalle aufteilen
      interval = np.linspace(0, 1, W+1)
  
      # Indices für jeweils a, b Kanäle berechnen
      indices = np.digitize(ab_image, interval) - 1
  
      # Bin Index berechnen
      bins = np.vectorize(calculate_bin)(indices[:,:,0], indices[:,:,1], W)
  
      return bins
  \end{minted}
  \captionof{lstlisting}{Binning eines normalisierten Lab Bildes}
\end{listing}

Für die Umwandlung werden vor dem Training alle Farben von den Trainingsbildern in Bins klassifiziert. Es wird ein Python Dictionary mit Bin
Indices als Key und ein 2-Dimensionales Array mit einem Array pro Farbkanal als Value erzeugt.

\begin{listing}[H]
  \begin{minted}{python}
    bin_colors = {i: [[], []] for i in range(n_bins)}
  \end{minted}
  \captionof{lstlisting}{Leere Dictionary Erzeugung für \textit{n\_bins}}
\end{listing}

Anschließend werden die Farben von jedem Farbkanal zu dem jeweiligen Bin Array zugeordnet. Als letztes wird der Durchschnitt pro Bin Index
und Farbkanal berechnet. Das Dictionary beinhaltet abschließend ein Array mit 2 einzelnen Werten für den jeweiligen Farbkanal pro Bin Index.
Somit kann unkompliziert ein Bin in eine Farbe umgewandelt werden.

\begin{listing}[H]
  \begin{minted}{python}
    a_color = bin_colors[bin_index][0]
    b_color = bin_colors[bin_index][1]
  \end{minted}
  \captionof{lstlisting}{``Bin-zu-Farbe'' Umwandlung}
\end{listing}

Der gleiche Vorgang wird für den Modus angewendet. Um einen Kompromiss zwischen Durchschnitt und Modus zu finden, werden der Durchschnitt
und der Modus mit einem Temperatur Wert interpoliert.

\begin{listing}[H]
  \begin{minted}{python}
    a_distance = a_mode - a_mean
    b_distance = b_mode - b_mean

    a = a_mode - (a_distance * T)
    b = b_mode - (b_distance * T)
  \end{minted}
  \captionof{lstlisting}{``Bin-zu-Farbe'' Berechnung mit einem Temperaturwert}
\end{listing}

\section{Datensätze}
Die Datensätze werden mit Hilfe der ``torchvision'' Bibliothek von PyTorch importiert und transformiert. Für das Importieren wurde ein
``ImageFolder'' implementiert, der die Bilder importiert, transformiert, normalisiert und die einzelenen Pixel in Bins klassifiziert.
Der Output der ``ImageFolder'' sind das Graustufenbild, das Bild mit den ``ab'' Farbkanälen und das in Bins kodierte Bild.

Mit Hilfe eines ``DataLoaders'' werden die Datensätze in Batches aufgeteilt und zufällig gemischt.

\section{Netzwerkarchitektur}
Für diese Arbeit wurden 2 U-Nets mit verschiedenen Größen verwendet. Ein U-Net für $ 32 \times 32 $ Input Bilder und ein U-Net für $ 128 \times 128 $
Input Bilder. Außerdem wurde das U-Net für $ 32 \times 32 $ Bilder angepasst, damit es mit einem MSE Loss verwendet werden kann. Bei der Anpassung wurde
das Output Volumen zu $ W_{Input} \times H_{Input} \times 2 $ geändert, wobei das Netzwerk direkt die Werte für die ``ab'' Farbkanäle vorhersagt.
Bei der Verwendung von einem MSE Loss wird das Binning nicht angewendet.

\subsection{ConvBlock}
Das Kernstück eines U-Nets ist der sogenannte ConvBlock. Ein ConvBlock beinhaltet zwei hintereinander geschaltete Blöcke, die wiederrum
aus einer Convolutional Layer mit $3 \times 3$ Filtern und Stride 1, gefolgt von ReLU und Batch Normalization bestehen.
Die Convolutional Layers verwenden Padding, um die Dimensionen des Inputs nicht zu verändern. Die Layers wurden mit den PyTorch
Klassen \textit{Conv2d}, \textit{BatchNorm2d} und der Funktion \textit{relu} implementiert. Der ConvBlock wird mit einer Klasse implementiert,
die wiederrum von der Oberklasse \textit{torch.nn.Module} erbt.

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{resources/networks/convblock.png}
  \caption{ConvBlock (Eigene Darstellung)}
  \label{image:convBlock}
\end{figure}

\subsection{U-Net}
Das U-Net wird mit den Spezifikationen aus \ref{section:u-net} implementiert. Dies geschieht mit einer Klasse, die von der Oberklasse
\textit{torch.nn.Module} erbt. Das U-Net verwendet neben den ConvBlocks, Max Pooling Layers mit einem $2 \times 2$ Filter und Stride 2,
um die Dimensionen in den Encoder zu halbieren. Die kleinste Größe der Feature Maps bei dem Encoder ist $8 \times 8$.
Der Decoder verwendet ebenfalls, neben den ConvBlocks, Transposed Convolutions mit $3 \times 3$
Filtern, Stride 2 und Padding 1, die die Größe der Feature Maps verdoppeln. Die letzte Layer ist eine Convolutional Layer mit $1 \times 1$ Filter,
Stride 1 und Padding 0, die die
Wahrscheinlichkeitsverteilung pro Pixel generiert. Alle Transposed Convolutions werden mit den Outputs der ConvBlocks mit den gleichen
Dimensionen aus dem Encoder konkateniert.
Es werden zwei verschieden große U-Nets implementiert, eins für $32 \times 32$ Bilder und eins für
$128 \times 128$ Bilder.
\\
Die Max Pooling Layers wurden mit der \textit{MaxPool2d} Klasse und die Transposed Convolutions mit der \textit{ConvTranspose2d} implementiert.

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{resources/networks/128u-net.png}
  \caption{U-Net Architektur für $128 \times 128$ Input Bilder (Eigene Darstellung)}
  \label{image:128u-net}
\end{figure}